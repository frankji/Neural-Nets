{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load packages and set up working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import os\n",
    "import time\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/Project/lstm_prediction/LSTM1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to standardize a matrix by columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(a):\n",
    "    nrow = a.shape[0]\n",
    "    mu = np.mean(a, axis=0, keepdims=True)\n",
    "    sig = np.std(a, axis=0, keepdims=True)\n",
    "    a = (a - np.repeat(mu, repeats=nrow, axis=0))/np.repeat(sig, repeats=nrow, axis=0)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ADHD = scipy.io.loadmat('/Project/lstm_prediction/ADHD_roimean.mat')\n",
    "adhd_roimean = ADHD['roimean']\n",
    "adhd_roimean = [standardize(m[0]) for m in adhd_roimean]\n",
    "adhd_response = standardize(np.squeeze(ADHD['ADHD_score'].T))\n",
    "#adhd_response = np.squeeze(ADHD['ADHD_score'].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ANT = scipy.io.loadmat('/Project/lstm_prediction/ANT_roimean.mat')\n",
    "ANT_rest = ANT['roimean_res']\n",
    "ANT_task = [m for m in ANT['roimean_task'][0]]\n",
    "ANT_response = ANT['ANTbehav']\n",
    "tmp_mean = np.repeat(np.array([np.mean(ANT['ANTbehav'], axis=0)]), axis=0, repeats=ANT['ANTbehav'].shape[0])\n",
    "tmp_std = np.repeat(np.array([np.std(ANT['ANTbehav'], axis=0)]), axis=0, repeats=ANT['ANTbehav'].shape[0])\n",
    "ANT_response = (ANT_response - tmp_mean) / tmp_std\n",
    "ANT_response = ANT_response[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regions = np.genfromtxt('/Project/lstm_prediction/labels_ADHD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdir = '/Project/lstm_prediction/gradCPTroimean/'\n",
    "gradCPT = scipy.io.loadmat('/Project/lstm_prediction/gradCPT_data_share.mat')\n",
    "gorder = np.genfromtxt('/Project/lstm_prediction/subject_order.txt', dtype='int')\n",
    "gradCPT_roimean = []\n",
    "for i in np.arange(len(gorder)):\n",
    "    fname = gdir + str(gorder[i]) + '_FullRest_matrix_bis_matrix_roimean.txt'\n",
    "    temp = np.genfromtxt(fname, delimiter='\\t', skip_header=True, )\n",
    "    temp = temp[:, 1:269]\n",
    "    temp = temp[:,regions!=21]\n",
    "    gradCPT_roimean.append(temp)\n",
    "gradCPT_response = np.squeeze(standardize(gradCPT['dprime']))\n",
    "#gradCPT_response = np.squeeze(gradCPT['dprime'].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic parameter setting.<br>\n",
    "num\\_steps: number of steps to trace<br>\n",
    "batch_size: number of sampels for each batch<br>\n",
    "state_size: number of states in GRU<br>\n",
    "num_layers: number of layers of multilayer GRU<br>\n",
    "learning_rate: step size in gradient descent<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 100\n",
    "batch_size = 100\n",
    "state_size = 256\n",
    "num_layers = 1\n",
    "learning_rate = [1e-5, 1e-5]\n",
    "input_x_dim = 268\n",
    "w_slice = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate batch with slice. If the slice size equals the step length, there will be no overlap between sampled time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_batch(raw_data_x, raw_data_y, num_steps, batch_size, w_slice = 1):\n",
    "    N = len(raw_data_x)\n",
    "    N_k = np.array([int((raw_data_x[i].shape[0] - num_steps) / w_slice)+1 for i in np.arange(N)])\n",
    "    epochs = np.concatenate([np.arange(0, n_k*w_slice, w_slice) for n_k in N_k])\n",
    "    Ns = np.concatenate([np.ones(y)*x for x, y in zip(np.arange(N), N_k)])\n",
    "    samples = np.array([[x, y] for x, y in zip(Ns, epochs)])\n",
    "    np.random.shuffle(samples)\n",
    "    epoch_size = samples.shape[0] // batch_size\n",
    "    samples = np.concatenate([samples, samples[0:(batch_size - samples.shape[0] + batch_size * epoch_size),]])\n",
    "    epoch_size = samples.shape[0] //batch_size\n",
    "    for i in range(epoch_size):\n",
    "        x = np.array([raw_data_x[int(samples[k, 0])]\\\n",
    "             [int(samples[k, 1]):(int(samples[k, 1]) + num_steps),] \\\n",
    "             for k in np.arange(i * batch_size, (i + 1) * batch_size)])\n",
    "        y = raw_data_y[np.int32(samples[np.arange(i * batch_size, (i + 1) * batch_size),0])]\n",
    "        z = np.int32(samples[np.arange(i * batch_size, (i + 1) * batch_size),0])\n",
    "        yield (x, y, z)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_epochs(num_epochs, raw_data_x, raw_data_y, num_steps, batch_size, w_slice=1):\n",
    "    for i in range(num_epochs):\n",
    "        yield gen_batch(raw_data_x, raw_data_y, num_steps, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to reset graph.<br>\n",
    "ref: https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average gradients for multi-tower design:\n",
    "https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "    # Note that each grad_and_vars looks like the following:\n",
    "    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "      # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "      # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "    # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(axis=0, values=grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "    # Keep in mind that the Variables are redundant because they are shared\n",
    "    # across towers. So .. we will just return the first tower's pointer to\n",
    "    # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to build computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(\n",
    "        state_size, \n",
    "        batch_size, \n",
    "        num_steps, \n",
    "        num_layers, \n",
    "        learning_rate,\n",
    "        input_x_dim,\n",
    "        hidden_states=256,\n",
    "        devices = ['/gpu:0'],\n",
    "        reg_scale=[0.001, 0.005],\n",
    "        keep_prob=[1, 0.5]):\n",
    "    reset_graph()\n",
    "    with tf.device('/cpu:0'):\n",
    "        x = tf.placeholder(tf.float32, [batch_size, num_steps, input_x_dim], name = 'fMRI')\n",
    "        y = tf.placeholder(tf.float32, [batch_size], name = 'Response')\n",
    "        cell = tf.contrib.rnn.GRUCell(state_size, activation=tf.nn.relu6)\n",
    "        cell_to_stack = tf.contrib.rnn.GRUCell(state_size, activation=tf.nn.relu6)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([cell] + [cell_to_stack] * (num_layers - 1), state_is_tuple = True)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_prob[0])\n",
    "        global_step = tf.get_variable('global_step', shape = [], trainable=False, initializer = tf.constant_initializer(0))\n",
    "        init_state = cell.zero_state(batch_size/len(devices), tf.float32)\n",
    "        x_splits = tf.split(x, num_or_size_splits=len(devices))\n",
    "        y_splits = tf.split(y, num_or_size_splits=len(devices))\n",
    "        lr1 = tf.train.exponential_decay(learning_rate[0],\n",
    "                                                 global_step = global_step,\n",
    "                                                 decay_steps = 30,\n",
    "                                                 decay_rate = 0.96,\n",
    "                                                 staircase=True)\n",
    "        lr2 = tf.train.exponential_decay(learning_rate[1],\n",
    "                                                 global_step = global_step,\n",
    "                                                 decay_steps = 30,\n",
    "                                                 decay_rate = 0.96,\n",
    "                                                 staircase=True)\n",
    "        opt1 = tf.train.AdamOptimizer(lr1)\n",
    "        opt2 = tf.train.AdamOptimizer(lr2)\n",
    "        with tf.variable_scope('Final'):\n",
    "            with tf.variable_scope('Dense1'):\n",
    "                W1 = tf.get_variable('W', [state_size,hidden_states])\n",
    "                b1 = tf.get_variable('bias', [hidden_states], initializer = tf.constant_initializer(0.0))\n",
    "            with tf.variable_scope('Dense2'):\n",
    "                W2 = tf.get_variable('W', [hidden_states, hidden_states])\n",
    "                b2 = tf.get_variable('bias', [hidden_states], initializer = tf.constant_initializer(0.0))                \n",
    "            with tf.variable_scope('output'):\n",
    "                Wo = tf.get_variable('W', [hidden_states, 1])\n",
    "                bo = tf.get_variable('bias', [1], initializer = tf.constant_initializer(0.0))\n",
    "        total_loss = []\n",
    "        outputs = []\n",
    "        grads1 = []\n",
    "        grads2 = []\n",
    "        l1_regularizer1 = tf.contrib.layers.l1_regularizer(scale=reg_scale[0], scope=None)\n",
    "        l1_regularizer2 = tf.contrib.layers.l1_regularizer(scale=reg_scale[1], scope=None)\n",
    "        for i in range(len(devices)):\n",
    "            d = devices[i]\n",
    "            with tf.device(d):   \n",
    "                rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, x_splits[i], initial_state = init_state)\n",
    "                \n",
    "                rnn_output = rnn_outputs[:,-1,:]                \n",
    "                dense1 = tf.nn.relu(tf.matmul(rnn_output, W1) + b1)\n",
    "                dense1 = tf.nn.dropout(dense1, keep_prob = keep_prob[1])    \n",
    "                dense2 = tf.nn.relu(tf.matmul(dense1, W2) + b2)\n",
    "                dense2 = tf.nn.dropout(dense2, keep_prob = keep_prob[1])    \n",
    "                output = tf.squeeze(tf.matmul(dense2, Wo) + bo)\n",
    "                outputs.append(output)\n",
    "                \n",
    "                w_output =[tf_var for tf_var in tf.trainable_variables() if not (\"rnn\" in tf_var.name or \"bias\" in tf_var.name)]\n",
    "                regularization_penalty_output = tf.contrib.layers.apply_regularization(l1_regularizer1, w_output)\n",
    "                w_rnn =[tf_var for tf_var in tf.trainable_variables() if not (\"Final\" in tf_var.name or \"bias\" in tf_var.name)]\n",
    "                regularization_penalty_rnn = tf.contrib.layers.apply_regularization(l1_regularizer2, w_rnn)\n",
    "                \n",
    "                loss = tf.reduce_mean(tf.abs(output - y_splits[i])) + regularization_penalty_output + regularization_penalty_rnn\n",
    "                grad1 = opt1.compute_gradients(loss, var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"rnn\"))\n",
    "                grad2 = opt2.compute_gradients(loss, var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"Final\"))\n",
    "                grads1.append(grad1)\n",
    "                grads2.append(grad2)\n",
    "                total_loss.append(loss)  \n",
    "        grads1 = average_gradients(grads1)\n",
    "        grads2 = average_gradients(grads2)\n",
    "        outputs = tf.concat(outputs, axis=0)\n",
    "        total_loss = tf.reduce_mean(total_loss)\n",
    "        \n",
    "        train_step = tf.group(opt1.apply_gradients(grads1, global_step = global_step), \n",
    "                              opt2.apply_gradients(grads2, global_step = global_step), \n",
    "                              tf.assign_add(global_step, 1))\n",
    "            \n",
    "    \n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        prediction = outputs,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph_simple(\n",
    "        state_size, \n",
    "        batch_size, \n",
    "        num_steps, \n",
    "        num_layers, \n",
    "        learning_rate,\n",
    "        input_x_dim,\n",
    "        reg_scale=[0.001, 0.005],\n",
    "        keep_prob=[1, 0.5]):\n",
    "    reset_graph()\n",
    "    with tf.device('/gpu:0'):\n",
    "        x = tf.placeholder(tf.float32, [batch_size, num_steps, input_x_dim], name = 'fMRI')\n",
    "        y = tf.placeholder(tf.float32, [batch_size], name = 'Response')\n",
    "        \n",
    "        global_step = tf.get_variable('global_step', shape = [], trainable=False, initializer = tf.constant_initializer(0))\n",
    "        cell = tf.contrib.rnn.GRUCell(state_size, activation = tf.nn.relu6)\n",
    "        cell_to_stack = tf.contrib.rnn.GRUCell(state_size, activation = tf.nn.relu6)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([cell] + [cell_to_stack] * (num_layers - 1), state_is_tuple = True)\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_prob[0])\n",
    "            \n",
    "        init_state = cell.zero_state(batch_size, tf.float32)\n",
    "        rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, x, initial_state = init_state)\n",
    "        \n",
    "        rnn_output = rnn_outputs[:,-1,:]\n",
    "        \n",
    "        with tf.variable_scope('Final', reuse = True):\n",
    "        \n",
    "            with tf.variable_scope('output'):\n",
    "                Wo = tf.get_variable('W', [state_size, 1])\n",
    "                bo = tf.get_variable('bias', [1], initializer = tf.constant_initializer(0.0))\n",
    "            \n",
    "    \n",
    "        dense3 = tf.nn.dropout(rnn_output, keep_prob = keep_prob[1])    \n",
    "        output = tf.squeeze(tf.matmul(dense3, Wo) + bo)\n",
    "        total_loss = tf.reduce_mean(tf.squared_difference(output, y))\n",
    "        l1_regularizer1 = tf.contrib.layers.l1_regularizer(scale=reg_scale[0], scope=None)\n",
    "        l1_regularizer2 = tf.contrib.layers.l1_regularizer(scale=reg_scale[1], scope=None)\n",
    "        w_output =[tf_var for tf_var in tf.trainable_variables() if not (\"rnn\" in tf_var.name or \"bias\" in tf_var.name)]\n",
    "        regularization_penalty_output = tf.contrib.layers.apply_regularization(l1_regularizer1, w_output)\n",
    "        w_rnn =[tf_var for tf_var in tf.trainable_variables() if not (\"Final\" in tf_var.name or \"bias\" in tf_var.name)]\n",
    "        regularization_penalty_rnn = tf.contrib.layers.apply_regularization(l1_regularizer2, w_rnn)    \n",
    "        regularized_loss = total_loss + regularization_penalty_output + regularization_penalty_rnn\n",
    "        vars1 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"rnn\")\n",
    "        vars2 = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"Final\")\n",
    "                \n",
    "        lr1 = tf.train.exponential_decay(learning_rate[0],\n",
    "                                         global_step = global_step,\n",
    "                                         decay_steps = 100,\n",
    "                                         decay_rate = 0.96,\n",
    "                                         staircase=True)\n",
    "        lr2 = tf.train.exponential_decay(learning_rate[1],\n",
    "                                         global_step = global_step,\n",
    "                                         decay_steps = 100,\n",
    "                                         decay_rate = 0.96,\n",
    "                                         staircase=True)\n",
    "\n",
    "        train_step1 = tf.train.AdamOptimizer(lr1).minimize(regularized_loss, var_list = vars1)\n",
    "        train_step2 = tf.train.AdamOptimizer(lr2).minimize(regularized_loss, var_list = vars2)\n",
    "        train_step = tf.group(train_step1, train_step2, tf.assign_add(global_step, 1))\n",
    "    \n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        prediction = output,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step,\n",
    "        var = [vars1, vars2]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(g, num_epochs, num_steps, \n",
    "                  batch_size, raw_data_x, raw_data_y, \n",
    "                  valid_data_x = None, valid_data_y = None, w_slice = 1,\n",
    "                  save = False, sess = None):\n",
    "    valid = False\n",
    "    if (valid_data_x is not None) and (valid_data_y is not None):\n",
    "        valid = True\n",
    "    if sess is None:\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    training_losses = []\n",
    "    for idx, epoch in enumerate(gen_epochs(num_epochs, raw_data_x, \n",
    "                                           raw_data_y, num_steps, batch_size, w_slice)):\n",
    "        training_loss = 0\n",
    "        steps = 0\n",
    "        flag = 0\n",
    "        for X, Y, Z in epoch:\n",
    "            steps = steps + 1\n",
    "            feed_dict = {g['x']: X, g['y']: Y}\n",
    "            training_loss_, _ = sess.run([g['total_loss'], g['train_step']],feed_dict)\n",
    "            if flag == 0:\n",
    "                flag = 1\n",
    "                if valid:\n",
    "                    valid_loss = 0\n",
    "                    valid_r = 0\n",
    "                    stepsv = 0\n",
    "                    valid_summary = []\n",
    "                    for Xv, Yv, Zv in gen_batch(valid_data_x, \n",
    "                                        valid_data_y, num_steps, batch_size, w_slice):\n",
    "                        valid_loss_, valid_pred = sess.run([g['total_loss'], g['prediction']], \n",
    "                                                         feed_dict = {g['x']: Xv, g['y']:Yv})\n",
    "                        valid_loss += valid_loss_\n",
    "                        valid_summary.append(np.array([valid_pred, Yv, Zv]))\n",
    "                        valid_r += np.corrcoef(valid_pred, Yv)[0,1]\n",
    "                        stepsv += 1\n",
    "                    valid_summary = np.concatenate(valid_summary, axis=1)\n",
    "                    summary = validsum(valid_summary.T)\n",
    "                    print('Average validation loss for Epoch', idx, \":\", valid_loss/stepsv)\n",
    "                    print('Average validation R for Epoch', idx, \":\", valid_r/stepsv)\n",
    "                    print('validation R for Epoch', idx, \":\", np.corrcoef(summary[:,0], summary[:,1])[0,1])\n",
    "            training_loss += training_loss_\n",
    "        print('Average training loss for Epoch', idx, \":\", training_loss/steps)\n",
    "    if isinstance(save, str):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, save)\n",
    "    return training_loss, valid_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be a predicted value for each time window. Average the predicted values of time windows corresponding to individuals and returen the summary matrix with prediction, response and individual label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validsum(valid_summary):\n",
    "    Z = valid_summary[:,2]\n",
    "    Y = valid_summary[:,1]\n",
    "    preds = valid_summary[:,0]\n",
    "    zs = np.unique(Z)\n",
    "    results=[]\n",
    "    for i in np.arange(len(zs)):\n",
    "        z = zs[i]\n",
    "        results.append([np.mean(preds[Z == z]), np.mean(Y[Z == z]), z])\n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the device information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time test for training 10 epochs using 4 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 3.484865665435791 seconds to build the graph.\n",
      "Average validation loss for Epoch 0 : 1.61791519324\n",
      "Average validation R for Epoch 0 : 0.106097355416\n",
      "validation R for Epoch 0 : 0.286413504124\n",
      "Average training loss for Epoch 0 : 1.17190149968\n",
      "Average validation loss for Epoch 1 : 1.22665141026\n",
      "Average validation R for Epoch 1 : -0.00491916869397\n",
      "validation R for Epoch 1 : -0.0725694246051\n",
      "Average training loss for Epoch 1 : 1.14686973479\n",
      "Average validation loss for Epoch 2 : 1.20344215631\n",
      "Average validation R for Epoch 2 : 0.0292518055246\n",
      "validation R for Epoch 2 : 0.107812127604\n",
      "Average training loss for Epoch 2 : 1.14369151156\n",
      "Average validation loss for Epoch 3 : 1.23790300886\n",
      "Average validation R for Epoch 3 : -0.0604159976538\n",
      "validation R for Epoch 3 : -0.220298838491\n",
      "Average training loss for Epoch 3 : 1.14475248939\n",
      "Average validation loss for Epoch 4 : 1.23049556216\n",
      "Average validation R for Epoch 4 : 0.00635642074072\n",
      "validation R for Epoch 4 : -0.0790651404598\n",
      "Average training loss for Epoch 4 : 1.14630047238\n",
      "Average validation loss for Epoch 5 : 1.2634375294\n",
      "Average validation R for Epoch 5 : -0.110393385737\n",
      "validation R for Epoch 5 : -0.408465684356\n",
      "Average training loss for Epoch 5 : 1.14432332917\n",
      "Average validation loss for Epoch 6 : 1.22394201159\n",
      "Average validation R for Epoch 6 : -0.0014475085657\n",
      "validation R for Epoch 6 : 0.0709655641136\n",
      "Average training loss for Epoch 6 : 1.14306622891\n",
      "Average validation loss for Epoch 7 : 1.26596309741\n",
      "Average validation R for Epoch 7 : -0.108520709958\n",
      "validation R for Epoch 7 : -0.457177086932\n",
      "Average training loss for Epoch 7 : 1.14468249629\n",
      "Average validation loss for Epoch 8 : 1.24916762114\n",
      "Average validation R for Epoch 8 : -0.0591196776117\n",
      "validation R for Epoch 8 : -0.207859051846\n",
      "Average training loss for Epoch 8 : 1.14477425493\n",
      "Average validation loss for Epoch 9 : 1.22587462266\n",
      "Average validation R for Epoch 9 : -0.0138832339105\n",
      "validation R for Epoch 9 : -0.177513525269\n",
      "Average training loss for Epoch 9 : 1.14278449709\n",
      "It took 621.9729232788086 seconds to train for 10 epochs.\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100\n",
    "batch_size = 32\n",
    "state_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = [1e-5, 1e-5]\n",
    "input_x_dim = 236\n",
    "w_slice = 40\n",
    "workdir = '/Project/models'\n",
    "os.chdir(workdir)\n",
    "t = time.time()\n",
    "g = build_graph(state_size = state_size,batch_size = batch_size, \n",
    "                num_steps = num_steps, num_layers = num_layers,\n",
    "                learning_rate = [1e-4, 1e-4], input_x_dim = input_x_dim, reg_scale = [1e-5, 1e-5], keep_prob = [0.4, 0.4], hidden_states=32, devices=['/gpu:0', '/gpu:1', '/gpu:2', '/gpu:3'])\n",
    "print('It took', time.time() - t, 'seconds to build the graph.')\n",
    "t = time.time()\n",
    "raw_data_x = np.array(adhd_roimean)\n",
    "raw_data_y = adhd_response\n",
    "valid_data_x = np.array(gradCPT_roimean)\n",
    "valid_data_y = gradCPT_response\n",
    "_, valid_summary = train_network(g, 10, num_steps, batch_size, raw_data_x = raw_data_x, raw_data_y = raw_data_y, \n",
    "              valid_data_x = valid_data_x, valid_data_y = valid_data_y, w_slice=w_slice, save = workdir + '/model.ckpt')\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 10 epochs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 3.6968748569488525 seconds to build the graph.\n",
      "INFO:tensorflow:Restoring parameters from /home/fas/zhao/dj333/Project/Imaging/share/lstm_prediction/LSTM1/tower/models/model.ckpt\n",
      "Average validation loss for Epoch 0 : 36.0225739913\n",
      "Average validation R for Epoch 0 : -0.028794941792\n",
      "Average training loss for Epoch 0 : 4.59227508617\n",
      "Average validation loss for Epoch 1 : 36.1441309958\n",
      "Average validation R for Epoch 1 : 0.0255554786764\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-c5bb0cadef0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkdir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/model.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m _, valid_summary = train_network(g, 10, num_steps, batch_size, raw_data_x = raw_data_x, raw_data_y = raw_data_y, \n\u001b[0;32m---> 20\u001b[0;31m               valid_data_x = valid_data_x, valid_data_y = valid_data_y, w_slice=w_slice, save = workdir + '/model1.ckpt', sess = sess)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"It took\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seconds to train for 10 epochs.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-ee7abfd176ef>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(g, num_epochs, num_steps, batch_size, raw_data_x, raw_data_y, valid_data_x, valid_data_y, w_slice, save, sess)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mtraining_loss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mflag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fas/zhao/dj333/softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fas/zhao/dj333/softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fas/zhao/dj333/softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fas/zhao/dj333/softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fas/zhao/dj333/softwares/Anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 60\n",
    "batch_size = 16\n",
    "state_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = [1e-5, 1e-5]\n",
    "input_x_dim = 236\n",
    "w_slice = 30\n",
    "workdir = '/Project/models'\n",
    "os.chdir(workdir)\n",
    "t = time.time()\n",
    "g = build_graph(state_size = state_size,batch_size = batch_size, \n",
    "                num_steps = num_steps, num_layers = num_layers,\n",
    "                learning_rate = [1e-5, 1e-5], input_x_dim = input_x_dim, reg_scale = [0.00001, 0.00001], keep_prob = [0.4, 0.4], hidden_states=256, devices=['/gpu:0', '/gpu:1', '/gpu:2', '/gpu:3'])\n",
    "print('It took', time.time() - t, 'seconds to build the graph.')\n",
    "t = time.time()\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "saver.restore(sess, workdir + '/model.ckpt')\n",
    "_, valid_summary = train_network(g, 10, num_steps, batch_size, raw_data_x = raw_data_x, raw_data_y = raw_data_y, \n",
    "              valid_data_x = valid_data_x, valid_data_y = valid_data_y, w_slice=w_slice, save = workdir + '/model1.ckpt', sess = sess)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 10 epochs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
